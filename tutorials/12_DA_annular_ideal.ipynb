{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TUTORIAL: \n",
    "# Data assimilation using real experimental data\n",
    "\n",
    "We can now put everything we have learned together. \n",
    "\n",
    "We can investigate two scenarios:\n",
    "\n",
    "A) Assume that we have access to the post-processed data and assimilate it. This situation simplifies the problem as the experimental data is not biased (see tutorial TA_azimuthal_data to see how the raw data is biased).\n",
    "-   Truth: post-processed data \n",
    "-   Observations: post-processed data + noise (possibly coloured noise)\n",
    "\n",
    "B) Assume a realistic setting in which the post-processed data is not available on the fly to input to the data assimilation algorithm. Here, we need to address the issue of biased observations.\n",
    "-   Truth: post-processed data\n",
    "-   Observations: raw data\n",
    "\n",
    "In this tutorial we will work with option A. For option B go to the tutorial ```11_DA_annular_raw.ipynb```."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "764b9fea212048bd"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from essentials.physical_models import Annular\n",
    "from essentials.bias_models import ESN\n",
    "import scipy.io as sio\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "\n",
    "if os.path.isdir('/mscott/'):\n",
    "    data_folder = '/mscott/an553/data/'  # set working directory to \n",
    "else:\n",
    "    data_folder = \"../data/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T11:40:40.927628Z",
     "start_time": "2024-02-23T11:40:40.606900Z"
    }
   },
   "id": "3ed20651c653ef43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load data \n",
    "Create the reference truth and the observations.\n",
    "\n",
    "The code below can be compacted with the function ```create_truth```\n",
    "```\n",
    "  \n",
    "from essentials.create import create_truth\n",
    "truth = create_truth(filename, t_start, t_stop, Nt_obs, std_obs, noise_type='gauss', post_processed=True)\n",
    "```\n",
    "\n",
    "The output ```truth``` is a dictionary containing\n",
    "```\n",
    "dict(y_raw=y_raw, y_true=y_true, t=t_true, dt=dt_t, t_obs=t_obs, y_obs=y_obs, \n",
    "     dt_obs=Nt_obs * dt_t, std_obs=std_obs)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13e856dabc8c870"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ER = 0.5625\n",
    "filename = data_folder + 'annular/ER_{}'.format(ER)\n",
    "\n",
    "# Load experimental data\n",
    "mat = sio.loadmat(filename + '.mat')\n",
    "y_true, t_true = [mat[key].squeeze() for key in ['y_filtered', 't']]\n",
    "\n",
    "i0, i1 = [np.argmin(abs(t_true - ti)) for ti in [3, 7]]\n",
    "y_true, t_true = [xx[i0:i1] for xx in [y_true, t_true]]\n",
    "t_true -= t_true[0]\n",
    "\n",
    "Nt, Nq = y_true.shape\n",
    "dt_t = t_true[1] - t_true[0]\n",
    "\n",
    "# Add noise to the truth to \"create\" the observations\n",
    "std_obs = 0.1\n",
    "noise = rng.multivariate_normal(np.zeros(Nq), np.eye(Nq) * std_obs ** 2, Nt)\n",
    "y_raw = y_true + noise * y_true\n",
    "\n",
    "# Select the observations time-window\n",
    "t_start = 2. * Annular.t_transient\n",
    "t_stop = t_start + Annular.t_CR * 10\n",
    "Nt_obs = 20\n",
    "\n",
    "# Sample the observations\n",
    "obs_idx = np.arange(t_start // dt_t, t_stop // dt_t + 1, Nt_obs, dtype=int)\n",
    "y_obs, t_obs = [xx[obs_idx] for xx in [y_raw, t_true]]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-23T11:40:40.942700Z"
    }
   },
   "id": "47e88dbbc92443e5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from essentials.plotResults import plot_truth\n",
    "plot_truth(y_true=y_true, y_raw=y_raw, t=t_true, t_obs=t_obs, y_obs=y_obs, dt=dt_t)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3343b64f3a25e224",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Define the forecast model\n",
    "This is the physical model which we will use to model the true data.\n",
    "Here, we select the filter parameters and create ensemble\n",
    "\n",
    "*The code below can be compacted as*\n",
    "```\n",
    "from essentials.create import create_ensemble\n",
    "ensemble = create_ensemble(model=Annular, **filter_params)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ffcdafa5e0a447e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "alpha0 = dict(nu=(40., 50.),\n",
    "              c2beta=(5, 20),\n",
    "              kappa=(1.E-4, 1.3E-4),\n",
    "              epsilon=(0.0001, 0.03),\n",
    "              omega=(1090 * 2 * np.pi, 1100 * 2 * np.pi),\n",
    "              theta_b=(0.5, 0.7),\n",
    "              theta_e=(0.5, 0.8)\n",
    "              )\n",
    "\n",
    "alpha0_mean = dict()\n",
    "for alpha, lims in alpha0.items():\n",
    "    alpha0_mean[alpha] = 0.5 * (lims[0] + lims[1])\n",
    "\n",
    "ensemble = Annular(**alpha0_mean)\n",
    "\n",
    "filter_params = {'m': 10, \n",
    "                 'inflation': 1.0,\n",
    "                 'std_psi': 0.3,\n",
    "                 'std_a': alpha0}\n",
    "\n",
    "# Forecast model to initialise the ensemble after transient\n",
    "state, t_ = ensemble.time_integrate(int(ensemble.t_CR / ensemble.dt))\n",
    "ensemble.update_history(state[-1], reset=True)\n",
    "\n",
    "ensemble.init_ensemble(**filter_params)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "846679d6cf1ed5e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Visualize ensemble initialization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a0b15f444bb3d50"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from essentials.plotResults import plot_ensemble\n",
    "plot_ensemble(ensemble, reference_params={'kappa': 1e-4, 'omega': 2 * np.pi})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7171305374a4a374",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Train an ESN to model the model bias\n",
    "The procedure is the following\n",
    "\n",
    "&emsp; i. Initialise ESN Bias class object\n",
    "&emsp; ii. Create synthetic bias to use as training data \n",
    "&emsp; iii. Train the ESN\n",
    "&emsp; iv. Create washout data\n",
    "\n",
    "<br><br>\n",
    "**4.1. Initialise the ESN**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9155103d82f09d8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from essentials.create import create_bias_training_dataset\n",
    "\n",
    "ensemble_no_bias = ensemble.copy()\n",
    "ensemble_ESN = ensemble.copy()\n",
    "\n",
    "train_params = dict(bias_model=ESN, \n",
    "                    upsample=5,\n",
    "                    N_units=50,\n",
    "                    N_wash=15,\n",
    "                    t_train=ensemble.t_CR * 10,\n",
    "                    t_test=ensemble.t_CR * 2,\n",
    "                    t_val=ensemble.t_CR * 2,\n",
    "                    # Training data generation options\n",
    "                    augment_data=True,\n",
    "                    bayesian_update=False,\n",
    "                    biased_observations=False,\n",
    "                    seed_W=0,\n",
    "                    L=10,\n",
    "                    # Hyperparameter search ranges\n",
    "                    rho_range=(0.5, 1.),\n",
    "                    sigma_in_range=(np.log10(1e-5), np.log10(1e1)),\n",
    "                    tikh_range=[1e-16, 1e-12, 1e-9]\n",
    "                    )\n",
    "ensemble_ESN.init_bias(**train_params)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c9a4acf4f81d58d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**4.2. Create training data**\n",
    "\n",
    "The details of the code inside ```create_bias_training_dataset()``` function is explained in the tutorial ```Class_Bias.ipynb```."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4337f57a7cebf3a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data = create_bias_training_dataset(y_raw, y_true, ensemble_ESN, **train_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdf8ed52ecae7fa9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize training data\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7e07941abd8d071"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#TODO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0f34ff303401986",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**4.3. Train the ESN**\n",
    "\n",
    "The training convergence, hyperparameter optimization and testing results are saved in a pdf file in *figs_ESN* folder."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e96d2997ea3b2075"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ensemble_ESN.bias.train_bias_model(**train_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddb04bc57eacba42",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4.4. Create washout data**\n",
    "\n",
    "We retrieve from the raw data a ```N_wash``` number of observations to use for initialising the ESN, i.e., to perform the washout. \n",
    "The ESN initialization must be before the fist observation.\n",
    "\n",
    "```\n",
    "from essentials.create import create_washout\n",
    "wash_t, wash_obs = create_washout(ensemble.bias, t=t_true, y_raw=y_raw)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1ba47fc4b7bffcb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ensemble_ESN.bias.t_init = t_obs[0] - 2 * Nt_obs * dt_t\n",
    "\n",
    "i1 = np.argmin(abs(ensemble_ESN.bias.t_init - t_true))\n",
    "i0 = i1 - ensemble_ESN.bias.N_wash  * ensemble_ESN.bias.upsample \n",
    "\n",
    "wash_obs = y_raw[i0:i1 + 1:ensemble_ESN.bias.upsample]\n",
    "wash_t = t_true[i0:i1 + 1:ensemble_ESN.bias.upsample]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "187c7776666ba83",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Apply data assimilation\n",
    "We now have all the ingredients to start our data assimilation algorithm."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0007caf0a985fcb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from essentials.DA import dataAssimilation\n",
    "\n",
    "DA_kwargs = dict(y_obs=y_obs, t_obs=t_obs, std_obs=std_obs, wash_obs=wash_obs, wash_t=wash_t)\n",
    "\n",
    "ensemble_ESN.filter = 'rBA_EnKF'\n",
    "ensemble_ESN.regularization_factor = 5.\n",
    "ensemble_no_bias.filter ='EnSRKF'\n",
    "\n",
    "\n",
    "out = []\n",
    "for ens in [ensemble_no_bias, ensemble_ESN]:\n",
    "# for ens in [ensemble_ESN]:\n",
    "    ens = ens.copy()\n",
    "    ens.t_init = t_obs[0]\n",
    "    ens.inflation = 1.001\n",
    "    \n",
    "    filter_ens = dataAssimilation(ens, **DA_kwargs.copy())\n",
    "    \n",
    "    #Forecast the ensemble further without assimilation\n",
    "    Nt_extra = int(filter_ens.t_CR / filter_ens.dt) * 5\n",
    "    \n",
    "    psi, t = filter_ens.time_integrate(Nt_extra)\n",
    "    filter_ens.update_history(psi, t)\n",
    "    \n",
    "    y = filter_ens.get_observable_hist(Nt_extra)\n",
    "    b, t_b = filter_ens.bias.time_integrate(t=t, y=y)\n",
    "    filter_ens.bias.update_history(b, t_b)\n",
    "    \n",
    "    out.append(filter_ens)\n",
    "    # out[-1] = filter_ens\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc1a30086c60e94c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from essentials.plotResults import plot_timeseries, plot_parameters, print_parameter_results\n",
    "\n",
    "\n",
    "truth = dict(y_raw=y_raw, y_true=y_true, t=t_true, dt=dt_t,\n",
    "             t_obs=t_obs, y_obs=y_obs, dt_obs=Nt_obs * dt_t,\n",
    "             std_obs=std_obs, wash_t=wash_t, wash_obs=wash_obs)\n",
    "\n",
    "\n",
    "print_parameter_results(out, )\n",
    "for filter_ens in out:\n",
    "    plot_timeseries(filter_ens, truth)\n",
    "    plot_parameters(filter_ens, truth)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ecd64970052a1eb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from essentials.plotResults import plot_states_PDF, plot_RMS_pdf\n",
    "plot_states_PDF(out, truth, nbins=20)\n",
    "plot_RMS_pdf(out, truth, nbins=20)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6c01d0311fe04f2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TODO: Plot ensemble spread?\n",
    "# TODO: Plot timeseries PSD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1910e1d016003fed",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
